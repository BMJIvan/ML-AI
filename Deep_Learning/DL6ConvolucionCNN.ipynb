{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-candidate1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python378jvsc74a57bd09e80d76ea6f7910366ea144e4925c6ce12227515ad9ee1266cb0780bd8ad6c40",
   "display_name": "Python 3.7.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "9e80d76ea6f7910366ea144e4925c6ce12227515ad9ee1266cb0780bd8ad6c40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"]=[5,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convoluciones \n",
    "#Es una operacion entre matrices donde una matriz peque√±a (kernel o filtro) se aplica de forma iterativa a los distintos trozos de una matriz mas grande (una imagen)\n",
    "\n",
    "#Se va a ver de forma practica el ejemplo de convolucion visto en la teoria. para ello creamos una imagen con dos trozos verticales, una blanco y otro negro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=64\n",
    "img_borde_vertical=numpy.zeros((img_size, img_size))\n",
    "\n",
    "img_borde_vertical[:,:int(img_size/2)]=1\n",
    "\n",
    "plt.imshow(img_borde_vertical, cmap=\"gray\")\n",
    "plt.title(\"Imagen original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora vamos a aplicarle un filtro de deteccion de bordes verticales (filtro sobel vertical) y ver el output. Para ello usaremos la funcion de scipy convolve2d que realiza la convolucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "filtro_sobel_vertical=numpy.array([[-1,0,1],[-1,0,1],[-1,0,1]])\n",
    "\n",
    "output_convolucion=signal.convolve2d(img_borde_vertical, filtro_sobel_vertical, mode=\"valid\")\n",
    "plt.imshow(numpy.absolute(output_convolucion), cmap=\"gray\")\n",
    "plt.title(\"Output de la convolucion con filtro vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora se va a cargar una imagen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "img_original=misc.ascent()\n",
    "\n",
    "plt.imshow(img_original, cmap=\"gray\")\n",
    "plt.title(\"imagen original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro_sobel_horizontal=numpy.array([[-1,-1,-1], [0,0,0], [1,1,1]])\n",
    "output_convolucion=signal.convolve2d(img_original, filtro_sobel_horizontal)\n",
    "plt.imshow(output_convolucion, cmap=\"gray\")\n",
    "plt.title(\"Output de la convolucion con filtro horinzontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redes Neuronales de convolucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datos\n",
    "#En este caso vamos a usar un dataset nuevo, el Fashion MNIST, que es un dataset mas moderno que el dataset de digitos MNIST, que hoy en dia se considera un dataset demasiado \"facil\", y que puede ser resuelto con modelos que no estan relacionados con la vision artificial (como los SVMs)\n",
    "\n",
    "#En concreto el dataset Fashion MNIST consiste en 60,000 fotos (28,28pixeles como el dataset de digitos) de prendas de ropa y su objetivo es clasificar las imagenes respecto al tipo de ropa mostrado (10 tipos distintos de articulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test)=fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se puede usar numpy.bincount para contar cuantas observaciones hay de cada clase de prenda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000],\n",
       "      dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "numpy.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos la variable objetivo a vectores one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train_one_hot=to_categorical(y_train)\n",
    "y_test_one_hot=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"]=[4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "@interact(i=IntSlider(min=0, max=100, step=1, value=1))\n",
    "def dibujar_imagen(i):\n",
    "    plt.imshow(x_train[i], cmap=\"gray\")\n",
    "    plt.title(\"clase de prenda: {}\".format(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#En primer lugar se va a usar una red densa y la vamos a evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplanamos las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_plano=x_train.reshape(x_train.shape[0], 28*28)\n",
    "x_test_plano=x_test.reshape(x_test.shape[0], 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "x_train_plano.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_2 (Dense)              (None, 128)               100480    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 256)               33024     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 167,690\nTrainable params: 167,690\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo_denso=Sequential()\n",
    "modelo_denso.add(Dense(128, activation=\"relu\", input_shape=(784,)))\n",
    "modelo_denso.add(Dropout(0.2))\n",
    "modelo_denso.add(Dense(256, activation=\"relu\"))\n",
    "modelo_denso.add(Dropout(0.2))\n",
    "modelo_denso.add(Dense(128, activation=\"relu\"))\n",
    "modelo_denso.add(Dropout(0.2))\n",
    "modelo_denso.add(Dense(numpy.unique(y_train).shape[0], activation=\"softmax\"))\n",
    "\n",
    "modelo_denso.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "modelo_denso.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "numpy.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4033 - accuracy: 0.8556\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.4033378064632416, 0.8555999994277954]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "modelo_denso.fit(x_train_plano, y_train_one_hot, epochs=30, batch_size=500, verbose=0)\n",
    "modelo_denso.evaluate(x_test_plano, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0.4033378064632416, 0.8555999994277954]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora vamos a usar una red de convolucion (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols=28, 28\n",
    "x_train=x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test=x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape=(img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 13, 13, 32)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 5408)              0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 32)                173088    \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 173,738\nTrainable params: 173,738\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, Flatten, MaxPooling2D\n",
    "\n",
    "batch_size=256\n",
    "num_classes=10\n",
    "epochs=10\n",
    "\n",
    "input_shape=(28,28,1)\n",
    "\n",
    "modelo_cnn=Sequential()\n",
    "modelo_cnn.add(Conv2D(32, kernel_size=(3,3), activation=\"relu\", input_shape=input_shape))\n",
    "modelo_cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "modelo_cnn.add(Dropout(0.3))\n",
    "modelo_cnn.add(Flatten())\n",
    "modelo_cnn.add(Dense(32, activation=\"relu\"))\n",
    "modelo_cnn.add(Dropout(0.5))\n",
    "modelo_cnn.add(Dense(num_classes, activation=\"softmax\"))\n",
    "modelo_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "60/60 [==============================] - 19s 300ms/step - loss: 4.9375 - accuracy: 0.1539\n",
      "Epoch 2/5\n",
      "60/60 [==============================] - 18s 304ms/step - loss: 2.1001 - accuracy: 0.1876\n",
      "Epoch 3/5\n",
      "60/60 [==============================] - 18s 304ms/step - loss: 2.0567 - accuracy: 0.1956\n",
      "Epoch 4/5\n",
      "60/60 [==============================] - 18s 302ms/step - loss: 1.9927 - accuracy: 0.2077\n",
      "Epoch 5/5\n",
      "60/60 [==============================] - 18s 305ms/step - loss: 1.9558 - accuracy: 0.2131\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f4be5bf048>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "modelo_cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "modelo_cnn.fit(x_train, y_train_one_hot, epochs=5, batch_size=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 1.5826 - accuracy: 0.3926\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1.582605004310608, 0.39259999990463257]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "modelo_cnn.evaluate(x_tes, y_test_one_hot, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[1.582605004310608, 0.39259999990463257]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_6 (Conv2D)            (None, 28, 28, 64)        640       \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 28, 28, 64)        256       \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 28, 28, 64)        0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 28, 28, 64)        36928     \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 28, 28, 64)        256       \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 14, 14, 64)        0         \n_________________________________________________________________\ndropout_13 (Dropout)         (None, 14, 14, 64)        0         \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 14, 14, 64)        36928     \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 14, 14, 64)        256       \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 7, 7, 64)          0         \n_________________________________________________________________\ndropout_14 (Dropout)         (None, 7, 7, 64)          0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 3136)              0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 500)               1568000   \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 500)               2000      \n_________________________________________________________________\ndropout_15 (Dropout)         (None, 500)               0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 10)                5010      \n=================================================================\nTotal params: 1,650,274\nTrainable params: 1,648,890\nNon-trainable params: 1,384\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "batch_size=128\n",
    "num_classes=10\n",
    "epochs=2\n",
    "filter_pixel=3\n",
    "noise=1\n",
    "droprate=0.35\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), padding=\"same\", activation=\"relu\", input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(droprate))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), activation=\"relu\", padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(droprate))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), activation=\"relu\", padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(droprate))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, use_bias=False, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(droprate))\n",
    "\n",
    "model.add(Dense(10,activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "191/469 [===========>..................] - ETA: 3:20 - loss: 0.8838 - accuracy: 0.7214"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-0470c10eaf95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callbacks=[\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_acc\",\n",
    "        patience=1,\n",
    "        mode=\"max\",\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history=model.fit(x_train, y_train_one_hot, batch_size=batch_size,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 14s 43ms/step - loss: 0.4509 - accuracy: 0.8357\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.45087411999702454, 0.8356999754905701]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test_one_hot, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0.45087411999702454, 0.8356999754905701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "#https://github.com/philipperemy/keras-visualize-activations/blob/master/read_activations.py\n",
    "def get_activations(model, model_inputs, print_shape_only=False, layer_name=None):\n",
    "    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "\n",
    "    model_multi_inputs_cond = True\n",
    "    if not isinstance(inp, list):\n",
    "        # only one input! let's wrap it in a list.\n",
    "        inp = [inp]\n",
    "        model_multi_inputs_cond = False\n",
    "\n",
    "    outputs = [layer.output for layer in model.layers if\n",
    "               layer.name == layer_name or layer_name is None]  # all layer outputs\n",
    "\n",
    "    funcs = [K.function(inp + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "\n",
    "    if model_multi_inputs_cond:\n",
    "        list_inputs = []\n",
    "        list_inputs.extend(model_inputs)\n",
    "        list_inputs.append(0.)\n",
    "    else:\n",
    "        list_inputs = [model_inputs, 0.]\n",
    "\n",
    "    layer_outputs = [func(list_inputs)[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "        if print_shape_only:\n",
    "            print(layer_activations.shape)\n",
    "        else:\n",
    "            print(layer_activations)\n",
    "    return activations\n",
    "\n",
    "def display_activations(activation_maps):\n",
    "    batch_size = activation_maps[0].shape[0]\n",
    "    assert batch_size == 1, 'One image at a time to visualize.'\n",
    "    for i, activation_map in enumerate(activation_maps):\n",
    "        print('Displaying activation map {}'.format(i))\n",
    "        shape = activation_map.shape\n",
    "        if len(shape) == 4:\n",
    "            print(shape)\n",
    "            activations = np.hstack(np.transpose(activation_map[0], (2, 0, 1)))\n",
    "        elif len(shape) == 2:\n",
    "            # try to make it square as much as possible. we can skip some activations.\n",
    "            activations = activation_map[0]\n",
    "            num_activations = len(activations)\n",
    "            if num_activations > 1024:  # too hard to display it on the screen.\n",
    "                square_param = int(np.floor(np.sqrt(num_activations)))\n",
    "                activations = activations[0: square_param * square_param]\n",
    "                activations = np.reshape(activations, (square_param, square_param))\n",
    "            else:\n",
    "                activations = np.expand_dims(activations, axis=0)\n",
    "        else:\n",
    "            raise Exception('len(shape) = 3 has not been implemented.')\n",
    "        plt.imshow(activations, interpolation='None', cmap='jet')\n",
    "        plt.show()"
   ]
  }
 ]
}